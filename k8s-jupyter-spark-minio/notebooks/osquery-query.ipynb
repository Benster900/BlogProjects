{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5588e74b-9f06-4d06-8a07-6d3236d78ed8",
   "metadata": {},
   "source": [
    "# Create PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb28b24b-e7d6-4f55-93e6-1d85bd36846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_ENDPOINT: spark://10.152.183.194:7077\n",
      "MINIO_ENDPOINT: 10.152.183.94:9000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "\n",
    "exampleJars = [\n",
    "    \"hadoop-aws-3.3.4.jar\",\n",
    "    \"aws-java-sdk-core-1.12.599.jar\",\n",
    "    \"aws-java-sdk-s3-1.12.599.jar\",\n",
    "    \"delta-storage-3.0.0.jar\",\n",
    "    \"delta-spark_2.12-3.0.0.jar\",\n",
    "    \"aws-java-sdk-dynamodb-1.12.599.jar\",\n",
    "    \"hadoop-common-3.3.4.jar\",\n",
    "]\n",
    "\n",
    "\n",
    "SPARK_ENDPOINT = str()\n",
    "for name, value in os.environ.items():\n",
    "    if name.endswith(\"SPARK_MASTER_SVC_SERVICE_HOST\"):\n",
    "        SPARK_ENDPOINT=f\"spark://{value}:7077\"\n",
    "print(f\"SPARK_ENDPOINT: {SPARK_ENDPOINT}\")\n",
    "\n",
    "\n",
    "MINIO_ENDPOINT = str()\n",
    "for name, value in os.environ.items():\n",
    "    if name.endswith(\"_MINIO_PORT\"):\n",
    "        MINIO_ENDPOINT=value.replace(\"tcp://\", \"\")\n",
    "print(f\"MINIO_ENDPOINT: {MINIO_ENDPOINT}\")\n",
    "\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"Myapp\")\n",
    "    # Sets the Spark master/captain URL to connect too.\n",
    "    .master(SPARK_ENDPOINT)\n",
    "    # JARs on disk to load into our Spark session\n",
    "    .config(\"spark.jars\", \",\".join(exampleJars))\n",
    "    # k8s service for Jupyter driver\n",
    "    .config(\"spark.driver.host\", \"jupyter-driver\")\n",
    "    # Port for Jupyter driver\n",
    "    .config(\"spark.driver.port\", 2222)\n",
    "    # Extending the capabilities of SQL searching with Delta tables\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    ####### AWS setup and creds #######\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"analyst\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"analyst123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"10000\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb69c6-08ab-4d94-9cce-64f5cc42688a",
   "metadata": {},
   "source": [
    "# Read the delta table into a dataframe\n",
    "\n",
    "To query the data we need to load the delta table data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab58f3b-bb44-4295-914c-6699382b1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://logs-silver/osquery/osquery.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc7ec2-a1bb-4ea5-92ba-87612f10ac0f",
   "metadata": {},
   "source": [
    "# Query delta table\n",
    "\n",
    "Below we are query the delta table for Osquery logs pertaining to the command line that contain the letter `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c2688-7660-4989-8d35-a7ce9e6bf493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- unixTime: long (nullable = true)\n",
      " |-- columns: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------------+----------------------------------------------------------+\n",
      "|hostname        |cmdline                                                   |\n",
      "+----------------+----------------------------------------------------------+\n",
      "|ip-172-16-55-120|/usr/lib/apt/apt-helper wait-online                       |\n",
      "|ip-172-16-55-120|/lib/systemd/systemd-networkd-wait-online -q --timeout=30 |\n",
      "|ip-172-16-55-120|/bin/sh /usr/lib/apt/apt.systemd.daily update             |\n",
      "|ip-172-16-55-120|/usr/bin/dpkg --print-foreign-architectures               |\n",
      "|ip-172-16-55-120|/bin/sh /usr/lib/apt/apt.systemd.daily lock_is_held update|\n",
      "|ip-172-16-55-120|cmp -s apt.extended_states.0 /var/lib/apt/extended_states |\n",
      "|ip-172-16-55-120|/bin/sh /usr/bin/which apt-config                         |\n",
      "|ip-172-16-55-120|apt-config shell AutoAptEnable APT::Periodic::Enable      |\n",
      "|ip-172-16-55-120|/usr/bin/dpkg --print-foreign-architectures               |\n",
      "|ip-172-16-55-120|apt-config shell VERBOSE APT::Periodic::Verbose           |\n",
      "+----------------+----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.where(df.table == \"process_events\") \\\n",
    "    .select(df.hostname, F.col(\"columns\").getItem(\"cmdline\").alias(\"cmdline\")) \\\n",
    "    .where(F.col(\"columns\").getItem(\"cmdline\").contains(\"b\")) \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f21f59-1323-4ba6-9dd9-abd00b886f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
